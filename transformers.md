# Transformers

## Architecture Deep Dive
- https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/
- https://arxiv.org/pdf/1706.03762
- https://github.com/tensorflow/tensor2tensor
- https://arxiv.org/abs/2004.05150
- https://github.com/allenai/longformer
- https://arxiv.org/abs/2007.14062
- https://github.com/google-research/bigbird
- https://arxiv.org/abs/2009.14794
- https://github.com/google-research/google-research/tree/master/performer
- https://arxiv.org/abs/2006.04768
- https://github.com/tatp22/linformer-pytorch



## Attention Mechanisms
- https://arxiv.org/pdf/1706.03762



## Transformers Basics
- https://arxiv.org/pdf/1706.03762
- https://github.com/tensorflow/tensor2tensor
- https://jalammar.github.io/illustrated-transformer/
- https://poloclub.github.io/transformer-explainer/
- https://nlp.seas.harvard.edu/annotated-transformer/
- https://github.com/harvardnlp/annotated-transformer
- https://www.datacamp.com/tutorial/how-transformers-work
- https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c
- https://github.com/evanmiller/LLM-Reading-List
- https://github.com/cmhungsteve/Awesome-Transformer-Attention
- https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/
- https://lilianweng.github.io/posts/2021-07-11-diffusion-models/



## Implementation Examples
- https://nlp.seas.harvard.edu/annotated-transformer/
- https://github.com/harvardnlp/annotated-transformer
- https://github.com/karpathy/build-nanogpt
- https://github.com/karpathy/minGPT
- https://github.com/huggingface/transformers
- https://github.com/Lightning-AI/lightning
- https://github.com/google/flax
- https://www.tensorflow.org/



## Multimodal Transformers
- https://arxiv.org/abs/2010.11929
- https://github.com/google-research/vision_transformer
- https://arxiv.org/abs/2103.14030
- https://github.com/microsoft/Swin-Transformer
- https://arxiv.org/abs/2103.00020
- https://github.com/openai/CLIP
- https://arxiv.org/abs/2204.14198
- https://github.com/deepmind/flamingo
- https://arxiv.org/abs/2303.03378
- https://arxiv.org/abs/2306.02858
- https://github.com/DAMO-NLP-SG/Video-LLaMA
- https://arxiv.org/abs/2102.12092
- https://github.com/openai/DALL-E
- https://arxiv.org/abs/2309.17421
- https://arxiv.org/abs/2212.04356
- https://github.com/openai/whisper
- https://arxiv.org/abs/2306.00347
- https://github.com/facebookresearch/audiocraft
- https://arxiv.org/abs/2301.11325
- https://github.com/google-research/google-research/tree/master/musiclm
- https://arxiv.org/abs/2306.12925
- https://arxiv.org/abs/2203.12602
- https://github.com/MCG-NJU/VideoMAE
- https://arxiv.org/abs/2102.03047
- https://github.com/facebookresearch/TimeSformer
- https://arxiv.org/abs/2306.05424
- https://github.com/microsoft/Video-ChatGPT



## Recent Transformer Architectures
- https://arxiv.org/abs/2312.00752
- https://github.com/state-spaces/mamba
- https://arxiv.org/abs/2110.01752
- https://github.com/HazyResearch/state-spaces
- https://arxiv.org/abs/2302.10866
- https://github.com/HazyResearch/hyena-dna
- https://arxiv.org/abs/2311.13194
- https://github.com/togethercomputer/stripedhyena
- https://arxiv.org/abs/2101.03961
- https://github.com/tensorflow/mesh
- https://arxiv.org/abs/2112.06905
- https://arxiv.org/abs/2401.04088
- https://github.com/mistralai/mistral-src
- https://x.ai/blog/grok-1
- https://arxiv.org/abs/2405.01030
- https://arxiv.org/abs/2403.17503
- https://www.anthropic.com/news/claude-3-5-sonnet
- https://arxiv.org/abs/2405.00550
- https://arxiv.org/abs/2307.02486
- https://github.com/microsoft/unilm
- https://arxiv.org/abs/2309.00071
- https://github.com/jquesnelle/yarn
- https://arxiv.org/abs/2005.11401
- https://github.com/facebookresearch/rag
- https://arxiv.org/abs/2301.12652
- https://github.com/microsoft/REPLUG
- https://arxiv.org/abs/2208.03299
- https://github.com/facebookresearch/atlas



## State Space Models
- https://arxiv.org/abs/2312.00752
- https://github.com/state-spaces/mamba
- https://arxiv.org/abs/2110.01752
- https://github.com/HazyResearch/state-spaces
- https://arxiv.org/abs/2011.04006
- https://github.com/google-research/long-range-arena
- https://arxiv.org/abs/2302.10866
- https://github.com/HazyResearch/hyena-dna
- https://arxiv.org/abs/2311.13194
- https://github.com/togethercomputer/stripedhyena
- https://arxiv.org/abs/2212.14052
- https://github.com/HazyResearch/h3


## Hardware Optimizations
- Training in many GPUs - https://lilianweng.github.io/posts/2021-09-25-train-large/ 
- Inference optimization Weng - https://lilianweng.github.io/posts/2023-01-10-inference-optimization/ 
- inference arithmetic - https://kipp.ly/transformer-inference-arithmetic/ 
- Parameter counting - https://kipp.ly/transformer-param-count/ 
- Flops computation - https://www.stat.cmu.edu/~ryantibs/convexopt-F18/scribes/Lecture_19.pdf 
- Fullstack inference optimization - https://yaofu.notion.site/Towards-100x-Speedup-Full-Stack-Transformer-Inference-Optimization-43124c3688e14cffaf2f1d6cbdf26c6c 
- Fullstack inference optimization2 - https://yaofu.notion.site/Full-Stack-Transformer-Inference-Optimization-Season-2-Deploying-Long-Context-Models-ee25d3a77ba14f73b8ae19147f77d5e2 
- Transformer family1.0 - https://lilianweng.github.io/posts/2018-06-24-attention/#a-family-of-attention-mechanisms 
- Transformer family2.0 - https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/ 



## Visual Guides
- https://jalammar.github.io/illustrated-transformer/
- https://poloclub.github.io/transformer-explainer/
- https://github.com/jessevig/bertviz
