# Hardware Inference

## Inference Optimization
- https://lilianweng.github.io/posts/2023-01-10-inference-optimization/
- https://yaofu.notion.site/Towards-100x-Speedup-Full-Stack-Transformer-Inference-Optimization-43124c3688e14cffaf2f1d6cbdf26c6c
- https://yaofu.notion.site/Full-Stack-Transformer-Inference-Optimization-Season-2-Deploying-Long-Context-Models-ee25d3a77ba14f73b8ae19147f77d5e2
- https://arxiv.org/abs/2307.07982
- https://astralord.github.io/posts/transformer-inference-optimization-toolset/
- https://www.stat.cmu.edu/~ryantibs/convexopt-F18/scribes/Lecture_19.pdf
- https://arxiv.org/pdf/2410.10989
- https://arxiv.org/pdf/2505.21487


## Kernel Fusion
- https://developer.nvidia.com/blog/delivering-the-missing-building-blocks-for-nvidia-cuda-kernel-fusion-in-python/
- https://developer.nvidia.com/blog/per-tensor-and-per-block-scaling-strategies-for-effective-fp8-training/
- https://github.com/deepseek-ai/DeepGEMM


## KV Caching
- https://arxiv.org/pdf/2501.01005
- https://arxiv.org/abs/2309.06180


## Multi-GPU Training
- https://lilianweng.github.io/posts/2021-09-25-train-large/


## Parameter Counting
- https://kipp.ly/transformer-param-count/
- https://kipp.ly/transformer-inference-arithmetic/


## Pruning
- https://arxiv.org/pdf/2405.05751
